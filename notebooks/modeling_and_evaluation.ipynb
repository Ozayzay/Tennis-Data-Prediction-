{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321871aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE MODELING - Decision Tree (No Feature Engineering)\n",
    "# ============================================================================\n",
    "\n",
    "# Purpose: Get a baseline performance metric before feature engineering\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d962e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Load Preprocessed Data\n",
    "# ============================================================================\n",
    "\n",
    "# Load the model-ready dataset , 124,097 matches)\n",
    "df = pd.read_csv('../data/processed/matches_with_engineered_features.csv')\n",
    "\n",
    "print(f\"âœ“ Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "print(df.columns.tolist()[:20], \"...\")  # Show first 20 columns\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(df['player1_won'].value_counts())\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# print all column names\n",
    "print(df.columns.tolist())\n",
    "df.tail(100).to_csv('df_tail.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7e8f2",
   "metadata": {},
   "source": [
    "# Trying things to improve accuracy :\n",
    "## 1. Deal with sparsity so only training on atp_matches and not qualifying matches since I believe those are more sparse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a55f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SPARSITY COMPARISON: Full Dataset vs ATP-Only\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load both datasets\n",
    "df_full = pd.read_csv('../data/processed/matches_with_engineered_features.csv')\n",
    "df_atp = df_full[df_full['Qualifying_match'] == 0].copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATASET SIZES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Full dataset:    {len(df_full):,} matches\")\n",
    "print(f\"ATP-only:        {len(df_atp):,} matches\")\n",
    "print(f\"Qualifying:      {(df_full['Qualifying_match'] == 1).sum():,} matches\")\n",
    "\n",
    "# Select engineered features to analyze\n",
    "engineered_features = [\n",
    "    'player1_career_matches', 'player2_career_matches',\n",
    "    'player1_career_win_rate', 'player2_career_win_rate',\n",
    "    'player1_clay_matches', 'player2_clay_matches',\n",
    "    'player1_grass_matches', 'player2_grass_matches',\n",
    "    'player1_hard_matches', 'player2_hard_matches',\n",
    "    'player1_clay_win_rate', 'player2_clay_win_rate',\n",
    "    'player1_grass_win_rate', 'player2_grass_win_rate',\n",
    "    'player1_hard_win_rate', 'player2_hard_win_rate',\n",
    "    'player1_recent_form', 'player2_recent_form',\n",
    "    'player1_h2h_win_rate', 'player2_h2h_win_rate',\n",
    "    'h2h_matches'\n",
    "]\n",
    "\n",
    "# 1. SPARSITY METRICS (% of zeros)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SPARSITY COMPARISON (% of Zeros)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sparsity_comparison = pd.DataFrame({\n",
    "    'Full_Dataset_Zero%': [(df_full[feat] == 0).mean() * 100 for feat in engineered_features],\n",
    "    'ATP_Only_Zero%': [(df_atp[feat] == 0).mean() * 100 for feat in engineered_features]\n",
    "}, index=engineered_features)\n",
    "\n",
    "sparsity_comparison['Improvement'] = sparsity_comparison['Full_Dataset_Zero%'] - sparsity_comparison['ATP_Only_Zero%']\n",
    "sparsity_comparison = sparsity_comparison.sort_values('Improvement', ascending=False)\n",
    "\n",
    "print(sparsity_comparison.to_string())\n",
    "\n",
    "# 2. MEAN/MEDIAN COMPARISON\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEAN VALUE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stats_comparison = pd.DataFrame({\n",
    "    'Full_Mean': [df_full[feat].mean() for feat in engineered_features],\n",
    "    'ATP_Mean': [df_atp[feat].mean() for feat in engineered_features],\n",
    "    'Full_Median': [df_full[feat].median() for feat in engineered_features],\n",
    "    'ATP_Median': [df_atp[feat].median() for feat in engineered_features]\n",
    "}, index=engineered_features)\n",
    "\n",
    "stats_comparison['Mean_Increase%'] = ((stats_comparison['ATP_Mean'] - stats_comparison['Full_Mean']) / \n",
    "                                       stats_comparison['Full_Mean'] * 100)\n",
    "stats_comparison = stats_comparison.sort_values('Mean_Increase%', ascending=False)\n",
    "\n",
    "print(stats_comparison.to_string())\n",
    "\n",
    "# 3. FEATURE COMPLETENESS SCORE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE COMPLETENESS (% of Non-Zero Values)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "completeness = pd.DataFrame({\n",
    "    'Full_Dataset': [(df_full[feat] != 0).mean() * 100 for feat in engineered_features],\n",
    "    'ATP_Only': [(df_atp[feat] != 0).mean() * 100 for feat in engineered_features]\n",
    "}, index=engineered_features)\n",
    "\n",
    "completeness['Improvement'] = completeness['ATP_Only'] - completeness['Full_Dataset']\n",
    "completeness = completeness.sort_values('Improvement', ascending=False)\n",
    "\n",
    "print(completeness.head(15).to_string())\n",
    "\n",
    "# 4. VISUALIZATIONS\n",
    "\n",
    "# Plot 1: Sparsity Heatmap\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Sparsity comparison bar chart\n",
    "top_10_sparse = sparsity_comparison.head(10)\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(top_10_sparse))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, top_10_sparse['Full_Dataset_Zero%'], width, label='Full Dataset', color='coral')\n",
    "ax.bar(x + width/2, top_10_sparse['ATP_Only_Zero%'], width, label='ATP Only', color='steelblue')\n",
    "ax.set_ylabel('% Zeros', fontweight='bold')\n",
    "ax.set_title('Top 10 Sparsest Features: Full vs ATP-Only', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top_10_sparse.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Mean value comparison\n",
    "top_10_mean = stats_comparison.head(10)\n",
    "ax = axes[0, 1]\n",
    "x = np.arange(len(top_10_mean))\n",
    "ax.bar(x - width/2, top_10_mean['Full_Mean'], width, label='Full Dataset', color='coral')\n",
    "ax.bar(x + width/2, top_10_mean['ATP_Mean'], width, label='ATP Only', color='steelblue')\n",
    "ax.set_ylabel('Mean Value', fontweight='bold')\n",
    "ax.set_title('Mean Values: Full vs ATP-Only', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top_10_mean.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Histogram comparison for key features\n",
    "ax = axes[1, 0]\n",
    "ax.hist(df_full['player1_career_matches'], bins=50, alpha=0.5, label='Full Dataset', color='coral', edgecolor='black')\n",
    "ax.hist(df_atp['player1_career_matches'], bins=50, alpha=0.5, label='ATP Only', color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Career Matches', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Career Matches Distribution Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Histogram for clay matches\n",
    "ax = axes[1, 1]\n",
    "ax.hist(df_full['player1_clay_matches'], bins=50, alpha=0.5, label='Full Dataset', color='coral', edgecolor='black')\n",
    "ax.hist(df_atp['player1_clay_matches'], bins=50, alpha=0.5, label='ATP Only', color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Clay Matches', fontweight='bold')\n",
    "ax.set_ylabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Clay Matches Distribution Comparison', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. SUMMARY STATISTICS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_sparsity_full = (df_full[engineered_features] == 0).mean().mean() * 100\n",
    "avg_sparsity_atp = (df_atp[engineered_features] == 0).mean().mean() * 100\n",
    "\n",
    "avg_completeness_full = (df_full[engineered_features] != 0).mean().mean() * 100\n",
    "avg_completeness_atp = (df_atp[engineered_features] != 0).mean().mean() * 100\n",
    "\n",
    "print(f\"Average Sparsity (% zeros):\")\n",
    "print(f\"  Full Dataset: {avg_sparsity_full:.2f}%\")\n",
    "print(f\"  ATP Only:     {avg_sparsity_atp:.2f}%\")\n",
    "print(f\"  Improvement:  {avg_sparsity_full - avg_sparsity_atp:+.2f}%\")\n",
    "\n",
    "print(f\"\\nAverage Completeness (% non-zeros):\")\n",
    "print(f\"  Full Dataset: {avg_completeness_full:.2f}%\")\n",
    "print(f\"  ATP Only:     {avg_completeness_atp:.2f}%\")\n",
    "print(f\"  Improvement:  {avg_completeness_atp - avg_completeness_full:+.2f}%\")\n",
    "\n",
    "print(f\"\\nMedian Career Matches:\")\n",
    "print(f\"  Full Dataset: {df_full['player1_career_matches'].median():.0f}\")\n",
    "print(f\"  ATP Only:     {df_atp['player1_career_matches'].median():.0f}\")\n",
    "print(f\"  Increase:     {df_atp['player1_career_matches'].median() - df_full['player1_career_matches'].median():+.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef580e9",
   "metadata": {},
   "source": [
    "### Only Using Atp_matches since less sparse in certain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6321a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Filter to ATP Main Tour Matches Only\n",
    "print(\"=\"*70)\n",
    "print(\"FILTERING TO ATP MAIN TOUR MATCHES ONLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"Original dataset: {len(df):,} matches\")\n",
    "print(f\"  Main ATP:     {(df['Qualifying_match'] == 0).sum():,}\")\n",
    "print(f\"  Qualifying:   {(df['Qualifying_match'] == 1).sum():,}\")\n",
    "\n",
    "# Store counts before filtering\n",
    "original_count = len(df)\n",
    "qualifying_count = (df['Qualifying_match'] == 1).sum()\n",
    "\n",
    "# Filter to ATP only\n",
    "df = df[df['Qualifying_match'] == 0].copy()\n",
    "\n",
    "print(f\"\\nFiltered dataset: {len(df):,} matches (ATP Main only)\")\n",
    "print(f\"Rows removed: {qualifying_count:,}\")\n",
    "\n",
    "# Drop the flag column (no longer needed)\n",
    "df = df.drop('Qualifying_match', axis=1)\n",
    "\n",
    "print(\"\\nâœ“ Filtered to ATP Main Tour matches\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Split Features and Target\n",
    "# ============================================================================\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "# AXIS EXPLANATION: \n",
    "X = df.drop('player1_won', axis=1)\n",
    "y = df['player1_won']\n",
    "\n",
    "print(f\"âœ“ Features and target separated\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {X.columns.tolist()[:10]}...\")  # Show first 10\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d693a21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d807e5",
   "metadata": {},
   "source": [
    "## Train test validation Split \n",
    "\n",
    "##### Think about: - the way I have made each row of my data its such that it doesn't have any identifying column , and the model should learn based off of what each match is about and the feature engineered values so Kfold shouldn't technically be an issue to do Kfold\n",
    "\n",
    "* !!! I unknowning did this but technically if I just did a Train test split based on %age that would lead into all the dates of the matches getting messed up which could potentially lead to data leakage since we have training on future data potentially and testing on data before it ? So potentially model is learning future \" trends or something \" and there is data leakage !!!!\n",
    "\n",
    "* #### Now what if I want to have those new trends in the recent data what should I do ? \n",
    "    * You still do the train test split based on the chronological order so there is no data leakage but then do this : \n",
    "\n",
    "    * Phase 1: Initial Training (2020-2022)\n",
    "    * Phase 2: Validation (2023)\n",
    "    * Phase 3: Test (2024)\n",
    "    * Phase 4: RETRAIN for Production (2025)\n",
    "        Now that 2024 is \"past\", include it in training!\n",
    "    * Deploy to predict 2025 matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b82f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Train/Validation/Test Split by Date\n",
    "# ============================================================================\n",
    "\n",
    "# Split based on tourney_date column\n",
    "# Train: 2018-2022 (dates < 20230000)\n",
    "# Validation: 2023 (20230000 <= dates < 20240000)\n",
    "# Test: 2024 (dates >= 20240000)\n",
    "\n",
    "# Split the dataframe first\n",
    "train_df = df[(df['tourney_date'] >= 20180000) & (df['tourney_date'] < 20230000)]\n",
    "val_df = df[(df['tourney_date'] >= 20230000) & (df['tourney_date'] < 20240000)].copy()\n",
    "test_df = df[df['tourney_date'] >= 20240000].copy()\n",
    "\n",
    "# Now separate X and y for each split, dropping tourney_date from features\n",
    "X_train = train_df.drop(['player1_won', 'tourney_date'], axis=1)\n",
    "y_train = train_df['player1_won']\n",
    "\n",
    "X_val = val_df.drop(['player1_won', 'tourney_date'], axis=1)\n",
    "y_val = val_df['player1_won']\n",
    "\n",
    "X_test = test_df.drop(['player1_won', 'tourney_date'], axis=1)\n",
    "y_test = test_df['player1_won']\n",
    "\n",
    "print(f\"âœ“ Data split by tournament date\")\n",
    "print(f\"\\nTrain set (2018-2022): {X_train.shape[0]:,} matches ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set (2023): {X_val.shape[0]:,} matches ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set (2024):       {X_test.shape[0]:,} matches ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFeatures per set: {X_train.shape[1]} (tourney_date dropped)\")\n",
    "print(f\"\\nClass balance in train set:\")\n",
    "print(y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb83333",
   "metadata": {},
   "source": [
    "# Hyper Parameter tuning using GridSearchCV [ Cross validation - no time based splitting here for hyper paramter tuning ]\n",
    "\n",
    "\n",
    "so when we defines the Clf= gridSearchCV { .....}\n",
    "\n",
    "clf is now a \"recipe\" that says:\n",
    "\n",
    "\"When someone calls .fit():\n",
    "  1. Take the RandomForestClassifier\n",
    "  2. Try it with ALL combinations in param_grid\n",
    "  3. Use 5-fold cross-validation for each\n",
    "  4. Score them using 'accuracy'\n",
    "  5. Print detailed progress (verbose=2)\n",
    "  6. Return the best one\"\n",
    "\n",
    "But NO TRAINING has happened yet!\n",
    "\n",
    "\n",
    "ðŸš€ Step 2: Calling .fit() (THIS is where the magic happens!)\n",
    "\n",
    "- it basically creates for loops and then with each parameter combo does the training and stores all the values that are important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "# For RandomForestClassifier\n",
    "# trained on a random subset of the data in terms of rows so each individual tree is trained on a random subset of the data \n",
    "# + Max_features determines the number of features considered for each split \n",
    "\n",
    "X_train_grid = pd.concat([X_train, X_val])\n",
    "Y_train_grid = pd.concat([y_train, y_val])\n",
    "\n",
    "print(f\"Training samples: {len(X_train_grid):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "\n",
    "# Defining the parameter Grid ( dictionary of parameters to tune )\n",
    "param_grid = { \n",
    "    'n_estimators' : [50, 100, 200, 300],\n",
    "    'max_depth' : [ 5, 8 , 15 , 20],\n",
    "    'min_samples_split' : [2, 5, 10, 20, 50],\n",
    "    #'min_samples_leaf' : [1, 2 ,4],\n",
    "    'max_features' : [.2 , .4 , .6 , .8 ]\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"\\nTotal combinations: {4*4*5*4} configs\")\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42 , n_jobs=-1)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# this creates a grid search object that will try all the combinations of the parameters in the param_grid\n",
    "# clf = GridSearchCV(\n",
    "#     estimator=rf_model,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     verbose=2, # detailed out put of the search\n",
    "#     n_jobs=-1 # use all available cores\n",
    "# )\n",
    "\n",
    "# # Perform GridSearchCV\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,          # how many random configs to try\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42)\n",
    "\n",
    "\n",
    "clf.fit(X_train_grid, Y_train_grid)\n",
    "\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOMIZED SEARCH RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Best params:\", clf.best_params_)\n",
    "print(f\"Best CV score: {clf.best_score_:.4f}\")\n",
    "print(f\"Test score: {clf.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# clf.cv_results_ is a pandas data frame that we can see in this format \n",
    "cv_results_df = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "print(\"\\nTop 20 configurations (RandomizedSearchCV):\")\n",
    "top_20 = cv_results_df.nlargest(20, 'mean_test_score')[\n",
    "    ['params', 'mean_test_score', 'rank_test_score']\n",
    "]\n",
    "for idx, row in top_20.iterrows():\n",
    "    print(f\"{int(row['rank_test_score']):2d}. {row['mean_test_score']:.4f} - {row['params']}\")\n",
    "\n",
    "top_20.to_csv('../results/rf_randomsearch_top20.csv', index=False)\n",
    "print(\"\\nâœ“ Top 20 configurations saved to '../results/rf_randomsearch_top20.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41284ca",
   "metadata": {},
   "source": [
    "## K-fold Vs Time based test train , validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d633634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT: K-Fold vs Time-Based Validation\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARING K-FOLD VS TIME-BASED VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Your baseline model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 1: K-Fold CV (What you're questioning)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š METHOD 1: K-Fold Cross-Validation (5-Fold)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Combine train+val for K-Fold\n",
    "X_train_val_combined = pd.concat([X_train, X_val])\n",
    "y_train_val_combined = pd.concat([y_train, y_val])\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold_scores = cross_val_score(\n",
    "    rf_model, \n",
    "    X_train_val_combined, \n",
    "    y_train_val_combined, \n",
    "    cv=kfold, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"K-Fold CV scores: {kfold_scores}\")\n",
    "print(f\"Mean CV score: {kfold_scores.mean():.4f} Â± {kfold_scores.std():.4f}\")\n",
    "\n",
    "# Now train on all train+val and test on 2024\n",
    "rf_model.fit(X_train_val_combined, y_train_val_combined)\n",
    "kfold_test_acc = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "print(f\"Test score (2024): {kfold_test_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# METHOD 2: Time-Based Validation (Proper way)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“Š METHOD 2: Time-Based Validation (2018-2022 train, 2023 val)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Train on 2018-2022 only\n",
    "rf_model_time = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model_time.fit(X_train, y_train)\n",
    "\n",
    "# Validate on 2023\n",
    "time_val_acc = accuracy_score(y_val, rf_model_time.predict(X_val))\n",
    "print(f\"Validation score (2023): {time_val_acc:.4f}\")\n",
    "\n",
    "# Now retrain on train+val and test on 2024\n",
    "rf_model_time.fit(X_train_val_combined, y_train_val_combined)\n",
    "time_test_acc = accuracy_score(y_test, rf_model_time.predict(X_test))\n",
    "print(f\"Test score (2024): {time_test_acc:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š COMPARISON RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nK-Fold CV:        {kfold_scores.mean():.4f} Â± {kfold_scores.std():.4f}\")\n",
    "print(f\"Time-Based Val:   {time_val_acc:.4f}\")\n",
    "\n",
    "cv_gap = kfold_scores.mean() - time_val_acc\n",
    "print(f\"\\nDifference: {cv_gap:+.4f} ({cv_gap*100:+.2f}%)\")\n",
    "\n",
    "if abs(cv_gap) < 0.005:  # Less than 0.5% difference\n",
    "    print(\"\\nâœ… MINIMAL DIFFERENCE - K-Fold is probably fine!\")\n",
    "    print(\"   Your point is valid: temporal leakage is negligible.\")\n",
    "elif abs(cv_gap) < 0.015:  # Less than 1.5% difference\n",
    "    print(\"\\nâš ï¸  SMALL DIFFERENCE - K-Fold slightly optimistic\")\n",
    "    print(\"   Some temporal leakage, but not severe.\")\n",
    "else:  # More than 1.5% difference\n",
    "    print(\"\\nâŒ SIGNIFICANT DIFFERENCE - K-Fold is too optimistic!\")\n",
    "    print(\"   Clear temporal leakage. Use time-based validation.\")\n",
    "\n",
    "print(f\"\\nTest scores (2024):\")\n",
    "print(f\"  K-Fold method:     {kfold_test_acc:.4f}\")\n",
    "print(f\"  Time-based method: {time_test_acc:.4f}\")\n",
    "print(f\"  Difference:        {(kfold_test_acc - time_test_acc):+.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL ANALYSIS: Feature Distributions Over Time\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ FEATURE DISTRIBUTION ANALYSIS (Do features shift over time?)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if key features have different distributions across years\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add year back temporarily for analysis\n",
    "train_df_with_year = df[(df['tourney_date'] >= 20180000) & \n",
    "                         (df['tourney_date'] < 20230000)].copy()\n",
    "train_df_with_year['year'] = train_df_with_year['tourney_date'] // 10000\n",
    "\n",
    "# Key features to check\n",
    "features_to_check = [\n",
    "    'rank_difference',\n",
    "    'career_win_rate_diff', \n",
    "    'recent_form_diff',\n",
    "    'player1_career_matches'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_to_check):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot distribution per year\n",
    "    for year in sorted(train_df_with_year['year'].unique()):\n",
    "        year_data = train_df_with_year[train_df_with_year['year'] == year][feature]\n",
    "        ax.hist(year_data, bins=30, alpha=0.4, label=str(year))\n",
    "    \n",
    "    ax.set_xlabel(feature, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature} Distribution by Year', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{feature}:\")\n",
    "    for year in sorted(train_df_with_year['year'].unique()):\n",
    "        year_data = train_df_with_year[train_df_with_year['year'] == year][feature]\n",
    "        print(f\"  {year}: mean={year_data.mean():.3f}, std={year_data.std():.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIf means/stds are similar across years â†’ K-Fold is safer\")\n",
    "print(\"If means/stds shift significantly â†’ Time-based validation is necessary\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446407b2",
   "metadata": {},
   "source": [
    "## Applying PCA ( Not improving - To be expected )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # After your train/val/test split\n",
    "\n",
    "# # 1. Standardize (MANDATORY for PCA!)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # 2. Apply PCA\n",
    "# pca = PCA(n_components=0.80)  # Keep 95% of variance\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_val_pca = pca.transform(X_val_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# print(f\"Reduced from {X_train.shape[1]} to {X_train_pca.shape[1]} features\")\n",
    "# print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# # # 3. Train models on PCA features\n",
    "\n",
    "# # ============================================================================\n",
    "# # STEP 4: Train All Models (WITH PCA)\n",
    "# # ============================================================================\n",
    "# # ============================================================================\n",
    "# # STEP 4: Train All Models (WITH PCA - Same Variable Names)\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"TRAINING ALL MODELS WITH PCA FEATURES\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # 1. Decision Tree (using PCA features)\n",
    "# print(\"\\n1. Training Decision Tree...\")\n",
    "# dt_model = DecisionTreeClassifier(\n",
    "#     max_depth=10,\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     random_state=42\n",
    "# )\n",
    "# dt_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# dt_y_train_pred = dt_model.predict(X_train_pca)\n",
    "# dt_y_val_pred = dt_model.predict(X_val_pca)\n",
    "# dt_y_test_pred = dt_model.predict(X_test_pca)\n",
    "# print(\"   âœ“ Decision Tree trained!\")\n",
    "\n",
    "# # 2. Random Forest (using PCA features)\n",
    "# print(\"\\n2. Training Random Forest...\")\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=100,\n",
    "#     max_depth=10,\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     random_state=42\n",
    "# )\n",
    "# rf_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# rf_y_train_pred = rf_model.predict(X_train_pca)\n",
    "# rf_y_val_pred = rf_model.predict(X_val_pca)\n",
    "# rf_y_test_pred = rf_model.predict(X_test_pca)\n",
    "# print(\"   âœ“ Random Forest trained!\")\n",
    "\n",
    "# # 3. Logistic Regression (using PCA features - already scaled!)\n",
    "# print(\"\\n3. Training Logistic Regression...\")\n",
    "# lr_model = LogisticRegression(\n",
    "#     max_iter=1000,\n",
    "#     random_state=42\n",
    "# )\n",
    "# lr_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# lr_y_train_pred = lr_model.predict(X_train_pca)\n",
    "# lr_y_val_pred = lr_model.predict(X_val_pca)\n",
    "# lr_y_test_pred = lr_model.predict(X_test_pca)\n",
    "# print(\"   âœ“ Logistic Regression trained!\")\n",
    "\n",
    "# # 4. XGBoost (using PCA features)\n",
    "# print(\"\\n4. Training XGBoost...\")\n",
    "# xgb_model = XGBClassifier(\n",
    "#     n_estimators=200,\n",
    "#     max_depth=8,\n",
    "#     learning_rate=0.1,\n",
    "#     subsample=0.58,\n",
    "#     colsample_bytree=0.8,\n",
    "#     random_state=42,\n",
    "#     eval_metric='logloss',\n",
    "#     use_label_encoder=False\n",
    "# )\n",
    "# xgb_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# xgb_y_train_pred = xgb_model.predict(X_train_pca)\n",
    "# xgb_y_val_pred = xgb_model.predict(X_val_pca)\n",
    "# xgb_y_test_pred = xgb_model.predict(X_test_pca)\n",
    "# print(\"   âœ“ XGBoost trained!\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"ALL MODELS TRAINED SUCCESSFULLY WITH PCA\")\n",
    "# print(f\"Features: {X_train_pca.shape[1]} principal components\")\n",
    "# print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706bb649",
   "metadata": {},
   "source": [
    "# Training All models for Non PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Train All Models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Decision Tree\n",
    "print(\"\\n1. Training Decision Tree...\")\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "dt_y_train_pred = dt_model.predict(X_train)\n",
    "dt_y_val_pred = dt_model.predict(X_val)\n",
    "dt_y_test_pred = dt_model.predict(X_test)\n",
    "print(\"   âœ“ Decision Tree trained!\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2. Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    # max_features=None,\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "rf_y_train_pred = rf_model.predict(X_train)\n",
    "rf_y_val_pred = rf_model.predict(X_val)\n",
    "rf_y_test_pred = rf_model.predict(X_test)\n",
    "print(\"   âœ“ Random Forest trained!\")\n",
    "\n",
    "# 3. Logistic Regression (needs scaling)\n",
    "print(\"\\n3. Training Logistic Regression...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_y_train_pred = lr_model.predict(X_train_scaled)\n",
    "lr_y_val_pred = lr_model.predict(X_val_scaled)\n",
    "lr_y_test_pred = lr_model.predict(X_test_scaled)\n",
    "print(\"   âœ“ Logistic Regression trained!\")\n",
    "\n",
    "# 4. XGBoost\n",
    "print(\"\\n4. Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.58,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "xgb_y_train_pred = xgb_model.predict(X_train)\n",
    "xgb_y_val_pred = xgb_model.predict(X_val)\n",
    "xgb_y_test_pred = xgb_model.predict(X_test)\n",
    "print(\"   âœ“ XGBoost trained!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL MODELS TRAINED SUCCESSFULLY\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b67f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Evaluate All Models\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Calculate and display evaluation metrics\"\"\"\n",
    "    # these functions are imported from sklearn.metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # print(f\"{dataset_name:15s}: Acc={accuracy:.4f} | Prec={precision:.4f} | Rec={recall:.4f} | F1={f1:.4f}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Store all models and predictions\n",
    "models = {\n",
    "    'Decision Tree': (dt_y_train_pred, dt_y_val_pred, dt_y_test_pred),\n",
    "    'Random Forest': (rf_y_train_pred, rf_y_val_pred, rf_y_test_pred),\n",
    "    'Logistic Reg':  (lr_y_train_pred, lr_y_val_pred, lr_y_test_pred),\n",
    "    'XGBoost':       (xgb_y_train_pred, xgb_y_val_pred, xgb_y_test_pred)\n",
    "}\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {}\n",
    "for model_name, (train_pred, val_pred, test_pred) in models.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    train_metrics = evaluate_model(y_train, train_pred, \"  Train\")\n",
    "    val_metrics = evaluate_model(y_val, val_pred, \"  Validation\")\n",
    "    test_metrics = evaluate_model(y_test, test_pred, \"  Test\")\n",
    "    \n",
    "    # Check overfitting\n",
    "    train_val_gap = (train_metrics[0] - val_metrics[0]) * 100\n",
    "    train_test_gap = (train_metrics[0] - test_metrics[0]) * 100\n",
    "    \n",
    "    # print(f\"  Overfit gap:   Train-Val={train_val_gap:+.2f}% | Train-Test={train_test_gap:+.2f}%\")\n",
    "    \n",
    "    # Store results\n",
    "    results[model_name] = {\n",
    "        'train_acc': train_metrics[0],\n",
    "        'val_acc': val_metrics[0],\n",
    "        'test_acc': test_metrics[0],\n",
    "        'train_val_gap': train_val_gap,\n",
    "        'train_test_gap': train_test_gap\n",
    "    }\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: TEST ACCURACY RANKING\")\n",
    "print(\"=\"*70)\n",
    "sorted_models = sorted(results.items(), key=lambda x: x[1]['test_acc'], reverse=True)\n",
    "\n",
    "for i, (model_name, metrics) in enumerate(sorted_models, 1):\n",
    "    print(f\"{i}. {model_name:15s}: {metrics['test_acc']:.4f} ({metrics['test_acc']*100:.2f}%)\")\n",
    "\n",
    "# Best model\n",
    "best_model = sorted_models[0][0]\n",
    "print(f\"\\nðŸ† Best Model: {best_model} with {sorted_models[0][1]['test_acc']*100:.2f}% test accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Feature Importance Analysis\n",
    "# ============================================================================\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_15 = feature_importance.head(15)\n",
    "plt.barh(range(len(top_15)), top_15['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_15)), top_15['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Top 15 Most Important Features - Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance to CSV\n",
    "feature_importance.to_csv('../results/baseline_dt_feature_importance.csv', index=False)\n",
    "print(\"\\nâœ“ Feature importance saved to '../results/baseline_dt_feature_importance.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4750f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
